\section{Text Mining und Datenstrukturen}
Um die oben beschrieben Eigenschaften visualisieren zu können muss eine entsprechende Datenbasis geschaffen werden, welches durch Text Mining ermöglicht wird. Der Prozess teilt sich dabei in folgende Schritte auf:
\begin{itemize}
\item Aufgabendefinition
\item Dokumentselektion
\item Dokumentaufbereitung
\item (Text) Mining Methoden
\item Interpretation / Evaluation
\item Anwendung
\end{itemize}
Die in der Einleitung beschrieben Motivation bildet die Aufgabendefinition dieses Projekts. Der nächste Schritt umfasst die Recherche und Sammlung von Dokumenten, welche dem Erreichen der vorher formulierten Ziele der Aufgabendefinition dienen. Oft werden dabei Dokumente aus verschiedenen Quellen mit verschiedenen Formaten entnommen.  Die Aufbereitung der Dokumente nimmt in einem Text Mining Vorhaben den größten Zeitaufwand ein, da hier Formate, Codierungen und Zeichen standardisiert werden müssen. Vor der Anwendung der Ergebnisse müssen die Daten noch interpretiert und evaluiert werden. Dabei kommen oft Experten mit entsprechenden Domänenwissen zum Einsatz, denn auch heute noch kommt Text Mining nicht komplett ohne menschliche Komponenten aus.

\subsection{Vorverarbeitung}
Da sich dieses Projekt primär auf die Visualisierung der Daten konzentriert wurde die Menge der Dokumente begrenzt und die die Vorverarbeitung entsprechend angepasst um zu Zeit sparen. \\
\\
Hierbei wurde aus Zeitgründen nur ein Dokument gewählt, welches die Möglichkeiten von Text Minung und Visualisierung verdeutlichen soll. Dieses wurde entsprechend mit Hilfe von XML standardisiert. So fällt ein Großteil der Zeit für die Sammlung von Dokumenten und dessen Standardisierung weg, welche in für die Visualisierung genutzt werden kann. \\
Durch diese Standardisierung lässt sich das Dokument nun auf mehreren Ebenen analysieren.

\subsubsection{Morphologisch}
Die morphologische Untersuchung behandelt die Texte auf Zeichenebene. Dabei werden folgende Verfahren auf Wortformen (sogenannte Token) angewendet. Hier einige Beispiele:\\

\begin{itemize}
\item Tokenisierung
\item Finden von Satzenden
\item Stammformreduktion (Stemming)
\end{itemize}
Üblicherweise beginnt man damit den Text in einzelne Wortformen (Token) zu zerlegen. Dabei wird in westlichen Sprachen üblicherweise das "white-space-tokenizing" verwendet. Dabei werden Strings anhand von Leerzeichen, Tabs, Zeilenumbrüchen und ähnlichen Trennzeichen aufgetrennt. Dabei treten erste Probleme auf, da nicht jede Sprache über Leerzeichen verfügt (Chinesisch) oder Wörter wie "New York" werden als einzelne Worte betrachtet, obwohl sie zusammen gehören.\\
\\
Der nächste Schritt umfasst das Finden von Satzgrenzen. Auch hier treten einige Herausforderungen auf, da beispielsweise nicht jeder Punkt ein Satzende darstellt. So bilden Abkürzungen wie "Dr." oder "et al." keine Satzgrenzen und müssen daher entsprechend dem Token zugeordnet werden.\\
\\
Ein weiterer Schritt ist das Stemming, welcher allerdings oft in Abhängigkeit der Text Mining Ziele durchgeführt wird. So gibt es hier einfache Normalisierungen wie das Auflösen von Ein- und Mehrzahl, welches man auch als "Inflectional Stemming" bezeichnet. Das Ziel dabei ist es die Genauigkeit der folgenden Analyse zu erhöhen, etwa bei statistischen Analysen wie Worthäufigkeiten. Als Gegensatz dazu existiert auch das "Root Stemming", welches eine aggressivere Form des Stemmings darstellt. Dabei geht das Stemming so weit, das Token wie "Application" auf ihren eigentlichen Wortstamm zurückgeführt werden ("apply").\\
\\
Da diese Untersuchungen oft die Grundlage von Text Mining Vorhaben bilden werden auch in diesem Projekt einige der oben genannten Methoden verwendet. So wurden die Texte tokenisiert und die Satzgrenzen entsprechend gekennzeichnet. \\
\\
Durch die Tokenisierung des Dokuments lassen sich die notwendigen Worthäufigkeiten (genauer: Token)  für die Visualisierung ermitteln. Zusätzlich kann dadurch die Satzlänge und die das Auftreten von Satzzeichen ermittelt werden, was die Daten für die Ermittlung der oben beschriebenen Komplexität liefert.

\subsubsection{Syntaktisch}
Die syntaktische Ebene befasst sich mit der Untersuchung von Texten auf Satzebene. Dabei werden die Zeichen in Beziehung zueinander gestellt und untersucht. Beispiele dafür sind folgende:\\
\begin{itemize}
\item Part-of-Speech
\item Phrase Recognition
\end{itemize}
Bei dem Part-of-Speech Tagging handelt es sich um das Erkennen von Wortklassen (Nomen, Verben, ...) und die Annotation der Token mit selbigen. Aufgrund der Vielfältigkeit von Sprache (und dem Umstand das eine Sprache lebt) werden hier oft statistische Ansätze verwendet.\\ 
\\
Bei der Phrase Recognition geht es darum Phrasen in Texten zu erkennen. So können POS-Tags dabei helfen bestimmte Wortgruppen zu finden, oder einfache Form wie das "Noun phrase chunking" ermöglichen.\\
\\
Wie bereits oben beschrieben liegt der Fokus auf diversen Messwerten, daher ist eine syntaktische Untersuchung der Texte in diesem Projekt nicht zielführend. Dennoch wurden mit Hilfe von Part-of-Speech Tagging versucht Werte für eine mögliche Erkennung des Nominalstils zu sammeln, welche im Ausblick diskutiert werden.

\subsubsection{Semantisch}
Die semantische Analyse befasst sich mit der Bedeutung von Wörtern und ihrer digitalen Darstellung. Hier wird oft auf Hintergrundwissen zurück gegriffen, so werden beispielsweise Fachsprachen (Informatik, Medizin, etc) über Ontologien oder Taxonomien modelliert und in den Prozess eingebunden. Damit sollen Probleme wie beispielsweise Mehrdeutigkeit (Word Sense Disambiguation) umgangen werden. \\
\\
Diese Art der Untersuchung erfordert allerdings einen größeren Umfang, als es in diesem Projekt möglich war, daher wurden keine semantischen Untersuchungen durchgeführt.

\subsection{Analyse}
Der Bereich Text und Data Mining umfasst eine große Mengen an Methoden um diverse Informationen aus Texten zu extrahieren bzw zugänglich zu machen. So werden diese oft in verschiedene Kategorien eingeteilt, wie beispielsweise das Information Retrieval oder Information Extraction. Diese Methoden sind sehr weitreichend und aufwendig. Eine entsprechende Vorstellung und Durchführung sind nicht Ziel dieses Projekts, daher wird an dieser Stelle darauf verzichtet.\\
\\
Stattdessen werden statistische Hilfsmittel genutzt um Daten für die Visualisierungen zu gewinnen. Dabei werden Mittel der deskriptiven Statistik verwendet um die in der Einleitung beschrieben Werte zu ermitteln.\\
\\
So ist insbesondere die Häufigkeit diverser Token ein wichtiger Messwert, welcher als Grundlage für eine qualitative Aussage dienen soll. Dabei handelt es sich um folgende einfache Formel:\\
\\
\centerline{$ tf(t,d) = f_{t,d} $}\\
\\
Dabei beschreibt $t$ den Term und und $d$ das Dokument. Da hier nur ein Dokument als Grundlage dient handelt es um eine einfache Summenfunktion:\\
\\
\centerline{$\sum_{i=1}^{n} t_i$}\\
\\
Neben diesen recht einfachen Werten sollen zusätzliche statistische Werte einen Überblick geben. So sollen Median und Durchschnitt zusätzliche Informationen anbieten, beispielsweise ein die durchschnittliche Satzlänge oder Verwendung von Satzzeichen.

\subsection{Datenstruktur und Implementierung}
Da die Visualisierung mit Hilfe von d3.js implementiert wird, werden die Daten in JSON bereitgestellt. Die Texte werden mit Hilfe von Open NLP der Apache Software Foundation aufbereitet, welches Java als Programmiersprache nutzt. Da das Dokument bereits in XML standardisiert ist, lässt es sich leicht mit Hilfe von JAXB und Data Binding in Java Objekte übersetzen. \\
\\
Open NLP stellt außerdem einige Modelle bereit um Aufgaben wie Tokenisierung oder Part-of-Speech Tagging durchzuführen. Die Daten werden im Anschluss mit einem Node.js Server gekoppelt um diverse Filteroptionen zu ermöglichen.\\
\\
In den folgenden Abschnitten werden die gesammelten Daten und deren Datenstrukturen im Detail beschrieben.
\newpage
\subsubsection*{Worthäufigkeit}
Die Worthäufigkeit wird durch durch ein Schlüssel-Wert-Paar dargestellt und mit Informationen über den Fundort angereichert. So wird nicht nur das Wort und die Anzahl gespeichert, sondern das Kapitel und die entsprechenden Unterkapitel mit gespeichert. Zusätzlich wurde der Wert in Abhängigkeit der gesamten Wortanzahl des Fundorts normalisiert. Daraus ergibt sich folgende Datenstruktur:

\begin{lstlisting}
{
  "word" : "visualization",
  "count" : 1,
  "chapterID" : 1,
  "sectionID" : 2,
  "subsectionID" : 0,
  "subsubsectionID" : 0,
  "normalized" : 0.4484304932735426
}
\end{lstlisting}

\subsubsection*{Satzlänge}
Die Sätzlänge wird ebenfalls durch die Häufigkeit dargestellt und mit ID's versehen. Hier erhält zusätzlich jeder Satz noch eine eigene ID und der Satz wird im Original mit gespeichert.

\begin{lstlisting}
{
  "chapterID" : 1,
  "sectionID" : 2,
  "subsectionID" : 0,
  "subsubsectionID" : 0,
  "sentenceID" : 135,
  "length" : 17,
  "sentence" : "Visual analytics (VA) is typically applied in scenarios where complex data has to be analyzed."
}
\end{lstlisting}

\subsubsection*{Satzzeichen}
Die Struktur Satzzeichen ist an der Struktur der Sätzlänge angelehnt. Der wichtige Unterschied ist das hier eine Liste der Position der Token mit angegeben wird um im Nachhinein zu aufzuzeigen, an welcher Stelle sich die Satzzeichen befinden.

\begin{lstlisting}
{
  "sentenceID" : 135,
  "count" : 2,
  "sentence" : "Visual analytics (VA) is typically applied in scenarios where complex data has to be analyzed.",
  "token" : [ 4, 17 ],
  "chapterID" : 1,
  "sectionID" : 2,
  "subsectionID" : 0,
  "subsubsectionID" : 0
}
\end{lstlisting}

\subsubsection*{Füllwörter}
Da die Füllwörter nicht durch einen einfachen Zähler dargestellt werden können unterscheidet sich diese Datenstruktur stark von den bisher vorgestellten. Da die Füllwörter pro Paragraph berechnet wurden erscheint hier eine zusätzliche ID. Auch eine Liste der Token ID's wurde übernommen, um die Position der Füllwörter aufzuzeigen. Da Paragraphen weder durch Text noch durch Zahlen eindeutig zu beschreiben sind werden hier auch die Titel der diversen Überschriften mit angegeben um so eine verständliche Zuordnung zu ermöglichen. 
\newpage
\begin{lstlisting}
{
  "paragraphID" : 3,
  "count" : 94,
  "token" : [ 5, 8, 10, 13, 14, 15, 20, 21, 22, 25, 26, 28, 29, 31, 32, 34, 35, 37, 39, 44, 47, 48, 51, 52, 54, 59, 62, 65, 67, 70, 71, 77, 79, 81, 82, 85, 86, 87, 89, 90, 92, 95, 100, 101, 103, 108, 110, 111, 114, 118, 120, 121, 123, 126, 128, 132, 133, 142, 143, 145, 146, 148, 150, 152, 154, 156, 159, 164, 168, 169, 171, 173, 176, 177, 181, 184, 188, 191, 196, 197, 198, 199, 200, 201, 203, 205, 206, 207, 208, 211, 214, 218, 219, 221 ],
  "chapterID" : 1,
  "sectionID" : 2,
  "subsectionID" : 0,
  "subsubsectionID" : 0,
  "chaptername" : "abstract",
  "sectionname" : null,
  "subsectionname" : null,
  "subsubsectionname" : null,
  "idInChapter" : 1
}
\end{lstlisting}